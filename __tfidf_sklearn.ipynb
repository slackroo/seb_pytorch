{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.109865Z",
     "start_time": "2025-07-11T03:19:42.090152Z"
    }
   },
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sumanth' -p scikit-learn,numpy,scipy -v -d"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Author: Sumanth\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.6\n",
      "IPython version      : 9.4.0\n",
      "\n",
      "scikit-learn: 1.7.0\n",
      "numpy       : 1.26.0\n",
      "scipy       : 1.16.0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tf-idf Walkthrough for scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I was using scikit-learn extremely handy `TfidfVectorizer` I had some trouble interpreting the results since they seem to be a little bit different from the standard convention of how the *term frequency-inverse document frequency* (tf-idf) is calculated. Here, I just put together a brief overview of how the `TfidfVectorizer` works -- mainly as personal reference sheet, but maybe it is useful to one or the other.\n",
    "m"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sections\n",
    "\n",
    "- [What are Tf-idfs all about?](#What-are-Tf-idfs-all-about?)\n",
    "- [Example data](#Example-data)\n",
    "- [Raw term frequency](#Raw-term-frequency)\n",
    "- [Normalized term frequency](#Normalized-term-frequency)\n",
    "- [Term frequency-inverse document frequency -- tf-idf](#Term-frequency-inverse-document-frequency----tf-idf)\n",
    "- [Inverse document frequency](#Inverse-document-frequency)\n",
    "- [Normalized tf-idf](#Normalized-tf-idf)\n",
    "- [Smooth idf](#Smooth-idf)\n",
    "- [Tf-idf in scikit-learn](#Tf-idf-in-scikit-learn)\n",
    "- [TfidfVectorizer defaults](#TfidfVectorizer-defaults)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## What are Tf-idfs all about?"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tf-idfs are a way to represent documents as feature vectors. Tf-idfs can be understood as a modification of the *raw term frequencies* (tf); the tf is the count of how often a particular word occurs in a given document. The concept behind the tf-idf is to downweight terms proportionally to the number of documents in which they occur. Here, the idea is that terms that occur in many different documents are likely unimportant or don't contain any useful information for Natural Language Processing tasks such as document classification."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the following sections, let us consider the following dataset that consists of 3 documents:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.167224Z",
     "start_time": "2025-07-11T03:19:42.161997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Raw term frequency"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we will start with the *raw term frequency* tf(t, d), which is defined by the  number of times a term *t* occurs in a document *t*\n",
    "\n",
    "<hr>\n",
    "Alternative term frequency definitions include the binary term frequency, log-scaled term frequency, and augmented term frequency [[1](#References)].\n",
    "<hr>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn, we can construct a bag-of-words model with the term frequencies as follows:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.222611Z",
     "start_time": "2025-07-11T03:19:42.198785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = cv.fit_transform(docs).toarray()\n",
    "tf"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.334580Z",
     "start_time": "2025-07-11T03:19:42.328443Z"
    }
   },
   "cell_type": "code",
   "source": "cv.vocabulary_",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Based on the vocabulary, the word \"and\" would be the 1st feature in each document vector in `tf`, the word \"is\" the 2nd, the word \"shining\" the 3rd, etc."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalized term frequency"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, we will take a brief look at how the tf-feature vector can be normalized, which will be useful later."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The most common way to normalize the raw term frequency is l2-normalization, i.e., dividing the raw term frequency vector $v$ by its length $||v||_2$ (L2- or Euclidean norm).\n",
    "\n",
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}} = \\frac{v}{\\big(\\sum_{i=1}^n v_i \\big)^{\\frac{1}{2}}}$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$\\frac{[1, 2, 1, 1, 1, 2, 1]}{2} = [ 0.2773501,  0.5547002,  0.2773501,  0.2773501,  0.2773501,\n",
    "        0.5547002,  0.2773501]$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.613510Z",
     "start_time": "2025-07-11T03:19:42.606935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_norm = tf[2] / np.sqrt(np.sum(tf[2]**2))\n",
    "tf_norm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2773501, 0.5547002, 0.2773501, 0.2773501, 0.2773501, 0.5547002,\n",
       "       0.2773501])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In scikit-learn, we can use the [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to normalize the term frequencies if we disable the inverse document frequency calculation (`use_idf: False` and `smooth_idf=False`):"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.723878Z",
     "start_time": "2025-07-11T03:19:42.709285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=False, norm='l2', smooth_idf=False, )\n",
    "tf_norm = tfidf.fit_transform(tf).toarray()\n",
    "print('Normalized term frequencies of document 3:\\n %s' % tf_norm[-1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized term frequencies of document 3:\n",
      " [0.2773501 0.5547002 0.2773501 0.2773501 0.2773501 0.5547002 0.2773501]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Term frequency-inverse document frequency -- tf-idf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most commonly, the term frequency-inverse document frequency (tf-idf) is calculated as follows [[1](#References)]:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t),$$\n",
    "where idf is the inverse document frequency, which we will introduce in the next section."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inverse document frequency"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to understand the *inverse document frequency* idf, let us first introduce the term *document frequency* $\\text{df}(d,t)$, which simply the number of documents $d$ that contain the term $t$. We can then define the idf as follows:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\\text{idf}(t) = log{\\frac{n_d}{1+\\text{df}(d,t)}},$$ \n",
    "where  \n",
    "$n_d$: The total number of documents  \n",
    "$\\text{df}(d,t)$: The number of documents that contain term $t$.\n",
    "\n",
    "Note that the constant 1 is added to the denominator to avoid a zero-division error if a term is not contained in any document in the test dataset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, Let us calculate the idfs of the words \"and\", \"is,\" and \"shining:\""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.791536Z",
     "start_time": "2025-07-11T03:19:42.786707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_docs = len(docs)\n",
    "\n",
    "df_and = 1\n",
    "idf_and = np.log(n_docs / (1 + df_and))\n",
    "print('idf \"and\": %s' % idf_and)\n",
    "\n",
    "df_is = 3\n",
    "idf_is = np.log(n_docs / (1 + df_is))\n",
    "print('idf \"is\": %s' % idf_is)\n",
    "\n",
    "df_shining = 2\n",
    "idf_shining = np.log(n_docs / (1 + df_shining))\n",
    "print('idf \"shining\": %s' % idf_shining)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf \"and\": 0.4054651081081644\n",
      "idf \"is\": -0.2876820724517809\n",
      "idf \"shining\": 0.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using those idfs, we can eventually calculate the tf-idfs for the 3rd document:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:42.870711Z",
     "start_time": "2025-07-11T03:19:42.865742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Tf-idfs in document 3:\\n')\n",
    "print('tf-idf \"and\": %s' % (1 * idf_and))\n",
    "print('tf-idf \"is\": %s' % (2 * idf_is))\n",
    "print('tf-idf \"shining\": %s' % (1 * idf_shining))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idfs in document 3:\n",
      "\n",
      "tf-idf \"and\": 0.4054651081081644\n",
      "tf-idf \"is\": -0.5753641449035618\n",
      "tf-idf \"shining\": 0.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tf-idf in scikit-learn"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The tf-idfs in scikit-learn are calculated a little bit differently. Here, the `+1` count is added to the idf, whereas instead of the denominator if the df:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$\\text{idf}(t) = log{\\frac{n_d}{\\text{df}(d,t)}} + 1$$ "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can demonstrate this by calculating the idfs manually using the equation above and comparing the results to the TfidfTransformer output using the settings `use_idf=True, smooth_idf=False, norm=None`.\n",
    "In the following examples, we will be again using the words \"and,\" \"is,\" and \"shining:\" from document 3."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.055799Z",
     "start_time": "2025-07-11T03:19:43.049584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_and = 1 \n",
    "df_and = 1 \n",
    "tf_and * (np.log(n_docs / df_and) + 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09861228866811"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.118276Z",
     "start_time": "2025-07-11T03:19:43.112655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_shining = 2 \n",
    "df_shining = 3 \n",
    "tf_shining * (np.log(n_docs / df_shining) + 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.221155Z",
     "start_time": "2025-07-11T03:19:43.214782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_is = 1 \n",
    "df_is = 2 \n",
    "tf_is * (np.log(n_docs / df_is) + 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.301883Z",
     "start_time": "2025-07-11T03:19:43.293367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm=None)\n",
    "tfidf.fit_transform(tf).toarray()[-1][0:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.        , 1.40546511])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalized tf-idf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let us calculate the normalized tf-idfs. Our feature vector of un-normalized tf-idfs for document 3 would look as follows if we'd applied the equation from the previous section to all words in the document:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.411256Z",
     "start_time": "2025-07-11T03:19:43.407604Z"
    }
   },
   "cell_type": "code",
   "source": "tf_idfs_d3 = np.array([[ 2.09861229, 2.0, 1.40546511, 1.40546511, 1.40546511, 2.0, 1.40546511]])",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the l2-norm, we then normalize the tf-idfs as follows:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.456272Z",
     "start_time": "2025-07-11T03:19:43.449246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_idfs_d3_norm = tf_idfs_d3[-1] / np.sqrt(np.sum(tf_idfs_d3[-1]**2))\n",
    "tf_idfs_d3_norm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46572049, 0.44383662, 0.31189844, 0.31189844, 0.31189844,\n",
       "       0.44383662, 0.31189844])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally, we compare the results to the results that the `TfidfTransformer` returns."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.536505Z",
     "start_time": "2025-07-11T03:19:43.529782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\n",
    "tfidf.fit_transform(tf).toarray()[-1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46572049, 0.44383662, 0.31189844, 0.31189844, 0.31189844,\n",
       "       0.44383662, 0.31189844])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Smooth idf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another parameter in the `TfidfTransformer` is the `smooth_idf`, which is described as\n",
    "> smooth_idf : boolean, default=True  \n",
    "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, our idf would then be defined as follows:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To confirm that we understand the `smooth_idf` parameter correctly, let us walk through the 3-word example again:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.627046Z",
     "start_time": "2025-07-11T03:19:43.619329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm=None)\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69314718, 2.        , 1.28768207])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.710824Z",
     "start_time": "2025-07-11T03:19:43.706130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_and = 1 \n",
    "df_and = 1 \n",
    "tf_and * (np.log((n_docs+1) / (df_and+1)) + 1) "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6931471805599454"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.761540Z",
     "start_time": "2025-07-11T03:19:43.756970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_is = 2\n",
    "df_is = 3 \n",
    "tf_is * (np.log((n_docs+1) / (df_is+1)) + 1) "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.820957Z",
     "start_time": "2025-07-11T03:19:43.816055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_shining = 1 \n",
    "df_shining = 2\n",
    "tf_shining * (np.log((n_docs+1) / (df_shining+1)) + 1)  "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2876820724517808"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TfidfVectorizer defaults"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we now understand the default settings in the `TfidfTransformer`, which are:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `use_idf=True`\n",
    "- `smooth_idf=True`\n",
    "- `norm='l2'`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And the equation can be summarized as   \n",
    "$\\text{tf-idf} =  \\text{tf}(t) \\times (\\text{idf}(t, d) + 1),$  \n",
    "where   \n",
    "$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}}.$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:22:56.740121Z",
     "start_time": "2025-07-11T03:22:56.734189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform(tf).toarray()[-1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.43370786, 0.        , 0.        , 0.55847784,\n",
       "       0.43370786, 0.55847784])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:43.932901Z",
     "start_time": "2025-07-11T03:19:43.928304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "smooth_tfidfs_d3 = np.array([[ 1.69314718, 2.0, 1.28768207, 1.28768207, 1.28768207, 2.0, 1.28768207]])\n",
    "smooth_tfidfs_d3_norm = smooth_tfidfs_d3[-1] / np.sqrt(np.sum(smooth_tfidfs_d3[-1]**2))\n",
    "smooth_tfidfs_d3_norm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40474829, 0.47810172, 0.30782151, 0.30782151, 0.30782151,\n",
       "       0.47810172, 0.30782151])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:24:53.347715Z",
     "start_time": "2025-07-11T03:24:53.339969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "v_tfidf = TfidfVectorizer(use_idf=True,norm='l2',smooth_idf=True,stop_words='english',)\n",
    "v_tfidf.fit_transform(docs).toarray()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.70710678, 0.70710678],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:24:54.632946Z",
     "start_time": "2025-07-11T03:24:54.627718Z"
    }
   },
   "cell_type": "code",
   "source": "v_tfidf.vocabulary_",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sun': 1, 'shining': 0, 'weather': 3, 'sweet': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " <br>\n",
    "<br>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### References"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[[back to top](#Sections)]"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[1] G. Salton and M. J. McGill. Introduction to modern information retrieval. 1983.\n",
    "[2] NB source: https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.1-working-with-text-data/\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's implement both in-memory and batch processing approaches for TF-IDF feature extraction, demonstrating when to use each:\n",
    "1. In-memory processing - for medium-sized datasets that fit in RAM\n",
    "2. Batch processing - for huge datasets that don't fit in memory"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:44.092139Z",
     "start_time": "2025-07-09T08:08:08.280797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:44.093701Z",
     "start_time": "2025-07-09T08:07:49.824615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Method 1: In-Memory TF-IDF Processing\n",
    "def process_in_memory(texts):\n",
    "        vectorizer = TfidfVectorizer(\n",
    "                max_features=10000,\n",
    "                stop_words='english',\n",
    "                min_df=5,\n",
    "                max_df=0.95,\n",
    "                ngram_range=(1, 2),\n",
    "                norm='l2'\n",
    "        )\n",
    "\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Get feature names and top terms\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_mean = X.mean(axis=0).A1\n",
    "        top_indices = tfidf_mean.argsort()[-10:][::-1]\n",
    "\n",
    "        print(\"In-Memory Processing Results:\")\n",
    "        print(f\"Documents: {X.shape[0]}, Features: {X.shape[1]}\")\n",
    "        print(f\"Sparsity: {100.0 * X.nnz / (X.shape[0] * X.shape[1]):.2f}%\")\n",
    "        print(\"\\nTop 10 terms by TF-IDF score:\")\n",
    "        for idx in top_indices:\n",
    "                print(f\"{feature_names[idx]}: {tfidf_mean[idx]:.4f}\")\n",
    "\n",
    "        return X, vectorizer"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:44.111921Z",
     "start_time": "2025-07-09T08:07:50.685175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Method 2: Batch Processing for Large Datasets\n",
    "def process_in_batches(texts, batch_size=1000, n_features=2 ** 18):\n",
    "        # Use HashingVectorizer for streaming\n",
    "        hasher = HashingVectorizer(\n",
    "                n_features=n_features,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),\n",
    "                norm='l2'\n",
    "        )\n",
    "\n",
    "        # Create directory for batch files\n",
    "        Path('batch_features').mkdir(exist_ok=True)\n",
    "\n",
    "        # Process batches\n",
    "        batch_files = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                batch_features = hasher.transform(batch)\n",
    "\n",
    "                # Save batch\n",
    "                batch_path = f'batch_features/batch_{i // batch_size}.npz'\n",
    "                sparse.save_npz(batch_path, batch_features)\n",
    "                batch_files.append(batch_path)\n",
    "\n",
    "                print(f\"Processed batch {i // batch_size + 1}, docs {i} to {i + len(batch)}\")\n",
    "\n",
    "        return batch_files, hasher"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:44.113608Z",
     "start_time": "2025-07-09T08:07:51.796521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_batches(feature_files):\n",
    "        first_batch = sparse.load_npz(feature_files[0])\n",
    "        all_features = sparse.csr_matrix((0, first_batch.shape[1]))\n",
    "\n",
    "        for file in feature_files:\n",
    "                batch = sparse.load_npz(file)\n",
    "                all_features = sparse.vstack([all_features, batch])\n",
    "\n",
    "        return all_features"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T03:19:44.114539Z",
     "start_time": "2025-07-09T08:08:11.463101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage with 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# 1. In-memory processing\n",
    "X_memory, vectorizer = process_in_memory(newsgroups.data[:1000])\n",
    "\n",
    "# 2. Batch processing\n",
    "batch_files, hasher = process_in_batches(newsgroups.data, batch_size=1000)\n",
    "X_batches = combine_batches(batch_files)\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"In-memory shape: {X_memory.shape}\")\n",
    "print(f\"Batch processing shape: {X_batches.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Memory Processing Results:\n",
      "Documents: 1000, Features: 2732\n",
      "Sparsity: 1.37%\n",
      "\n",
      "Top 10 terms by TF-IDF score:\n",
      "like: 0.0196\n",
      "know: 0.0192\n",
      "just: 0.0179\n",
      "don: 0.0177\n",
      "people: 0.0173\n",
      "think: 0.0153\n",
      "use: 0.0146\n",
      "does: 0.0136\n",
      "time: 0.0128\n",
      "thanks: 0.0126\n",
      "Processed batch 1, docs 0 to 1000\n",
      "Processed batch 2, docs 1000 to 2000\n",
      "Processed batch 3, docs 2000 to 3000\n",
      "Processed batch 4, docs 3000 to 4000\n",
      "Processed batch 5, docs 4000 to 5000\n",
      "Processed batch 6, docs 5000 to 6000\n",
      "Processed batch 7, docs 6000 to 7000\n",
      "Processed batch 8, docs 7000 to 8000\n",
      "Processed batch 9, docs 8000 to 9000\n",
      "Processed batch 10, docs 9000 to 10000\n",
      "Processed batch 11, docs 10000 to 11000\n",
      "Processed batch 12, docs 11000 to 12000\n",
      "Processed batch 13, docs 12000 to 13000\n",
      "Processed batch 14, docs 13000 to 14000\n",
      "Processed batch 15, docs 14000 to 15000\n",
      "Processed batch 16, docs 15000 to 16000\n",
      "Processed batch 17, docs 16000 to 17000\n",
      "Processed batch 18, docs 17000 to 18000\n",
      "Processed batch 19, docs 18000 to 18846\n",
      "\n",
      "Comparison:\n",
      "In-memory shape: (1000, 2732)\n",
      "Batch processing shape: (18846, 262144)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Key differences between the implementations:\n",
    "\n",
    "1. In-memory (TfidfVectorizer):\n",
    "   - Keeps vocabulary in memory\n",
    "   - Provides feature names\n",
    "   - Better for feature interpretation\n",
    "   - Limited by RAM\n",
    "\n",
    "2. Batch (HashingVectorizer):\n",
    "   - No vocabulary storage\n",
    "   - Fixed feature dimension\n",
    "   - Handles huge datasets\n",
    "   - Saves intermediate results\n",
    "   - Memory efficient\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:48:07.856056Z",
     "start_time": "2025-07-14T02:48:05.882452Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:51:47.522969Z",
     "start_time": "2025-07-14T02:51:47.517502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "torch.finfo(a.dtype)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
